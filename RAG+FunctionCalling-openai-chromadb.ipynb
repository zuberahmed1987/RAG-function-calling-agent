{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Retrieval Augmented Generation (RAG) Framework to contextualize the response of a chatbot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q python-dotenv langchain_elasticsearch langchain_community langchain_text_splitters langchain_core langchain langchain_openai langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.tools import tool\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.agents import  AgentExecutor, create_react_agent\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "#from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "#from langchain_elasticsearch.vectorstores import ElasticsearchStore\n",
    "from langchain_chroma import Chroma\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load_dotenv(find_dotenv(), override=True)\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"http://10.35.151.101:8001/v1\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-1234\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VectorDB setup and add embeddings to vector db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"dgxa100-user-guide.pdf\")#, extract_images=True)\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024,\n",
    "                                      chunk_overlap=512,\n",
    "                                      length_function=len,\n",
    "                                      is_separator_regex=False,\n",
    "                                      )\n",
    "\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "#embedding = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "#vector_db = ElasticsearchStore.from_documents(\n",
    "#    docs,\n",
    "#    es_url=\"http://localhost:9200\",\n",
    "#    index_name=\"manual\",\n",
    "#    embedding=embedding\n",
    "#)\n",
    "#vector_db.client.indices.refresh(index=\"manual\")\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "vector_db = Chroma.from_documents(documents=docs, embedding=embedding, persist_directory=\"./vectordb\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting vector db to LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever = vector_db.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"llama3-8b\",temperature=0.2)\n",
    "\n",
    "#llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "Keep your answers short and concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        #(\"system\", qa_system_prompt),\n",
    "        (\"system\", \"{context}\"),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask a question how to configure multi-instance GPU?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To configure Multi-Instance GPU (MIG) on an NVIDIA DGX A100, follow these steps:\n",
      "\n",
      "1. Stop the NVSM and DCGM services:\n",
      "```\n",
      "$ sudo systemctl stop nvsm dcgm\n",
      "```\n",
      "2. Enable MIG on all eight GPUs in the system:\n",
      "```\n",
      "$ sudo nvidia-smi -mig 1\n",
      "```\n",
      "If other services are running that prevent you from resetting the GPUs, then reboot the system and skip the next step.\n",
      "\n",
      "3. Restart the DCGM and NVSM services:\n",
      "```\n",
      "$ sudo systemctl start dcgm nvsm\n",
      "```\n",
      "Note: Before enabling MIG, make sure to terminate any system services that manage GPUs, as MIG requires a GPU reset.\n",
      "\n",
      "After enabling MIG, you can use the NVIDIA Management Library (NVML) APIs or the command-line utility `nvidia-smi` to manage MIG instances. You can also refer to the MIG User Guide for more detailed information on key MIG concepts, deployment considerations, and how to create MIG instances and run Docker containers using MIG.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask a question how to update dgx a100 software?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the NVIDIA DGX A100 User Guide, you can update the software on your DGX A100 system by following these steps:\n",
      "\n",
      "1. Run the package manager:\n",
      "```\n",
      "$ sudo apt update\n",
      "```\n",
      "2. Check to see which software will get updated:\n",
      "```\n",
      "$ sudo apt full-upgrade -s\n",
      "```\n",
      "3. Upgrade to the latest version:\n",
      "```\n",
      "$ sudo apt full-upgrade\n",
      "```\n",
      "4. Answer any questions that appear. Most questions require a Yes or No response. If asked to select the grub configuration to use, select the default option.\n",
      "\n",
      "Additionally, if you have configured apt to use the NVIDIA DGX OS packages in the file `/etc/apt/sources.list.d/dgx-bionic-r450-cuda11-0-repo.list`, the NVIDIA graphics driver will be upgraded to the R450 driver and the package sources will be updated to obtain future updates from the R450 driver repositories.\n",
      "\n",
      "Note: These instructions update all software for which updates are available from your configured software sources, including applications that you installed yourself. If you want to prevent an application from being updated, you can instruct the Ubuntu package manager to keep the current version. Refer to the Introduction for more information.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask a question \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems like you didn't ask a question. If you have any questions or need further assistance with updating the software on your DGX A100 system, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "i = 0\n",
    "while i < 3:\n",
    "    question = input(\"Ask a question\")\n",
    "    response = rag_chain.invoke({\"input\":question, \"chat_history\":chat_history})\n",
    "    chat_history.extend([HumanMessage(content=question), response[\"answer\"]])\n",
    "    print(response[\"answer\"])\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending to Function Calling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Vector DB search retrieval as a tool.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tool\n",
    "def query_manual(query):\n",
    "    \"\"\"\n",
    "    Queries the manual and retrieves information from its contents. \n",
    "    Returns the result and the source documents.\n",
    "\n",
    "    Args:\n",
    "        query (string): A query derived from the question asked by the user.\n",
    "    \"\"\"\n",
    "    result = qa.invoke(query)\n",
    "    return result['result'], result['source_documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "template = \"\"\"\n",
    "You are an assistant that answers questions on the manual provided.\n",
    "Use the tools provided to respond accurately. \n",
    "The query_manual tool should be used to retrieve information from the manual.\n",
    "\n",
    "For questions that require further information, use the tavily_search_tool_json tool to conduct research and respond\n",
    "with accurate answers.\n",
    "\n",
    "Question: {input}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "agentprompt = hub.pull(\"hwchase17/react-chat\")\n",
    "\n",
    "#tools = [query_manual, TavilySearchResults(max_results=3)]\n",
    "tools = [query_manual]\n",
    "\n",
    "agent = create_react_agent(llm=llm,\n",
    "                           tools=tools,\n",
    "                           prompt=agentprompt)\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent,\n",
    "                               tools=tools,\n",
    "                               handle_parsing_errors=True,\n",
    "                               verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['agent_scratchpad', 'chat_history', 'input', 'tool_names', 'tools'] metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'react-chat', 'lc_hub_commit_hash': '3ecd5f710db438a9cf3773c57d6ac8951eefd2cd9a9b2a0026a65a0893b86a6e'} template='Assistant is a large language model trained by OpenAI.\\n\\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\\n\\nTOOLS:\\n------\\n\\nAssistant has access to the following tools:\\n\\n{tools}\\n\\nTo use a tool, please use the following format:\\n\\n```\\nThought: Do I need to use a tool? Yes\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n```\\n\\nWhen you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\\n\\n```\\nThought: Do I need to use a tool? No\\nFinal Answer: [your response here]\\n```\\n\\nBegin!\\n\\nPrevious conversation history:\\n{chat_history}\\n\\nNew input: {input}\\n{agent_scratchpad}'\n"
     ]
    }
   ],
   "source": [
    "print(agentprompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "i = 0\n",
    "while i < 3:\n",
    "    question = input(\"Ask a question\")\n",
    "    response = agent_executor.invoke({\"input\":prompt_template.format(input=question), \"chat_history\": chat_history})\n",
    "    chat_history.extend({\"user\": question, \"ai\": response[\"output\"]})\n",
    "    print(response['output'])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
